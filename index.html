<!DOCTYPE html>
<html lang="en">
<head>
    <title>Ye He (何晔) </title>
    <meta name="description" content="Ye He GitHub Pages site.">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ye He (何晔) </title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Ye He (何晔)</h1>
            <nav>
                <ul>
                    <li><a href="#home">Home</a></li>
                    <li><a href="#publications">Publications</a></li>
                    <li><a href="#teaching">Teaching</a></li>
                    <li><a href="#contact">Contact</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <section id="home" class="home">
        <div class="container">
            <img src="your-photo.jpeg" alt="Your Photo" class="profile-photo">
            <div class="intro">
                <h2>Welcome to my personal website!</h2>
                <p>I am a Hale Visiting Assistant Professor in the <a href="https://math.gatech.edu/" target="_blank">School of Mathematics</a>, Georgia Institute of Technology, hosted by <a href="https://mtao8.math.gatech.edu/" target="_blank">Prof. Molei Tao</a>. Before joining Georgia Institute of Technology, I received my PhD. degree in Mathematics from University of California, Davis advised by <a href="https://sites.google.com/view/kriznakumar/home" target="_blank">Prof. Krishna Balasubramanian</a>. My research focuses on the mathematical foundations of artificial intelligence, machine learning and data science, with an emphasis on developing scalable inference methods, including sampling, diffusion models and stochastic optimization.</p>
                <p>Starting Fall 2026, I will be joining the Department of Applied Mathematics at the University of Colorado Boulder as a tenure-track Assistant Professor.</p>
                <a href="cv.pdf" class="btn">Download CV</a>
            </div>
        </div>
    </section>

    <section id="publications" class="publications">
        <div class="container">
            <h2>Publications</h2>
            <ul>
                <li>
                    Kevin Rojas, <strong>Ye He,</strong> Chieh-Hsin Lai, Yuta Takida, Yuki Mitsufuji, and Molei Tao (2025). Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models. <em>Preprint.</em> <a href="https://arxiv.org/abs/2507.08965" target="_blank">Link</a>
                </li>  
                <li>
                    <strong>Ye He,</strong> Kevin Rojas, and Molei Tao (2025). What Exactly Does Guidance Do in Masked Discrete Diffusion Models. <em>Preprint.</em> <a href="https://arxiv.org/abs/2506.10971" target="_blank">Link</a>
                </li>  
                <li>
                    Yuqing Wang, <strong>Ye He,</strong> and Molei Tao (2024). Evaluating the Design Space of Diffusion-based Generative Models. <em>NeurIPS 2024.</em> <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/227404a13d20898dec2018ebe368b202-Abstract-Conference.html" target="_blank">Link</a>
                </li>
                <li>
                    <strong>Ye He,</strong> Alireza Mousavi-Hosseini, Krishnakumar Balasubramanian, and Murat A. Erdogdu (2024). A separation in Heavy-tailed Sampling: Gaussian vs. Stable Oracles for Proximal Samplers. <em>NeurIPS 2024.</em> <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/77f33e8bd80345de4aea8554bbe5a4da-Abstract-Conference.html" target="_blank">Link</a>
                </li>
                <li>
                    <strong>Ye He,</strong> Kevin Rojas, and Molei Tao (2024). Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoting Diffusion. <em>NeurIPS 2024.</em> <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/8337b176767d948275d0b6d17cf4221e-Abstract-Conference.html" target="_blank">Link</a>
                </li>
                <li>
                    Krishnakumar Balasubramanian, and Promit Ghosal, and <strong>Ye He,</strong> (2024. Authors listed alphabetically). High-dimensional Scaling Limits and Fluctuations of Online Least-Squares SGD with smooth Covariance. <em>Annals of Applied Probability.</em> <a href="https://projecteuclid.org/journals/annals-of-applied-probability/volume-35/issue-5/High-dimensional-scaling-limits-and-fluctuations-of-online-least-squares/10.1214/24-AAP2123.short" target="_blank">Link</a>
                </li>
                <li>
                    Alireza Mousavi-Hosseini, Tyler K. Farghly, <strong>Ye He,</strong> Krishnakumar Balasubramanian, and Murat A. Erdogdu (2023). Towards a Complete Analysis of Langevin Monte Carlo: Beyond Poincare inequality. <em>COLT 2023.</em> <a href="https://proceedings.mlr.press/v195/mousavi-hosseini23a/mousavi-hosseini23a.pdf" target="_blank">Link</a>
                </li>
                <li>
                    <strong>Ye He,</strong> Krishnakumar Balasubramanian, and Murat A. Erdogdu (2022). An analysis of Transformed Unadjusted Langevin Algorithm for Heavy-tailed Sampling. <em>IEEE transactions on Information Theory.</em> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258391" target="_blank">Link</a>
                </li>
               <li>
                    <strong>Ye He,</strong> Krishnakumar Balasubramanian, Bharath K. Sriperumbudur, and Jianfeng Lu (2022). Regularized Stein Variational Gradient Flow. <em>Foundations of Computational Mathematics.</em> <a href="https://link.springer.com/article/10.1007/s10208-024-09663-w" target="_blank">Link</a>
                </li>
               <li>
                    <strong>Ye He,</strong> Tyler K. Farghly, Krishnakumar Balasubramanian, and Murat A. Erdogdu (2022). Mean-Square Analysis of Discretized Ito Diffusions for Heavy-Tailed Sampling. <em>Journal of Machine Learning Research.</em> <a href="https://www.jmlr.org/papers/v25/22-1198.html" target="_blank">Link</a>
                </li>
                <li>
                    <strong>Ye He,</strong> Krishnakumar Balasubramanian, and Murat A. Erdogdu (2020). On the Ergodicity, Bias and Asymptotic Normality of Randomized Midpoint Sampling Method. <em>NeurIPS 2020</em> <a href="https://proceedings.neurips.cc/paper/2020/hash/5265d33c184af566aeb7ef8afd0b9b03-Abstract.html" target="_blank">Link</a>
                </li>
            </ul>
        </div>
    </section>

    <section id="teaching" class="teaching">
        <div class="container">
            <h2>Teaching</h2>
            <h3>Courses Taught</h3>
            <p> Summer 2019: <a href="https://catalog.ucdavis.edu/courses-subject-code/mat/"> MAT016C, Short Calculus</a></p>
            <p> Fall 2023: <a href="https://catalog.gatech.edu/courses-undergrad/math/"> MATH1553, Introduction to Linear Algebra</a></p> 
            <p> Spring 2024: <a href="https://catalog.gatech.edu/courses-undergrad/math/"> MATH2552, Differential Equations</a></p>
        </div>
    </section>

    <section id="contact" class="contact">
        <div class="container">
            <h2>Contact</h2>
            <p>Email: <a href="mailto:yhe367@gatech.edu">yhe367@gatech.edu</a></p>
            <p>Office: <a href="https://avservices.gatech.edu/building/002/">Skiles</a> 016</p>
            <p>Social: 
                <a href="https://scholar.google.com/citations?user=PC25rDIAAAAJ&hl=en&oi=sra">Google Scholar</a>
            </p>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2024 Dr. Ye He. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
